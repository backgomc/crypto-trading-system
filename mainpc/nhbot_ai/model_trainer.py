# íŒŒì¼ ê²½ë¡œ: mainpc/nhbot_ai/model_trainer.py
# ì½”ë“œëª…: AI ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (ìŠ¤ë ˆë“œ ì•ˆì „ ë²„ì „)

import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pickle
import json
from datetime import datetime
from pathlib import Path
import threading
import time
from typing import Dict, List, Optional, Tuple, Callable

class ModelTrainer:
   """AI ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›)"""
   
   def __init__(self, symbol: str = "BTCUSDT"):
       self.symbol = symbol
       # âŒ DataCollectorë¥¼ ì—¬ê¸°ì„œ ìƒì„±í•˜ì§€ ì•ŠìŒ (ìŠ¤ë ˆë“œ ë¬¸ì œ ë°©ì§€)
       # self.data_collector = DataCollector(symbol)
       
       # í•™ìŠµ ìƒíƒœ ê´€ë¦¬
       self.is_training = False
       self.training_thread = None
       self.training_status = {
           "status": "idle",  # idle, running, completed, failed
           "current_epoch": 0,
           "total_epochs": 0,
           "current_batch": 0,
           "total_batches": 0,
           "metrics": {
               "loss": 0.0,
               "accuracy": 0.0,
               "val_loss": 0.0,
               "val_accuracy": 0.0
           },
           "start_time": None,
           "progress_callback": None
       }
       
       # ğŸ†• ì§€í‘œ ë§¤í•‘ ì¬êµ¬ì„± (í•µì‹¬/ì„ íƒ ë¶„ë¦¬)
       self.essential_indicators = {
           # í•µì‹¬ ì§€í‘œ (ì¶”ì„¸ ì „í™˜ íŒë‹¨)
           "price": ["close", "price_change", "hl_range"],
           "macd": ["macd", "macd_signal", "macd_histogram"],
           "rsi": ["rsi_14"],
           "bb": ["bb_position", "bb_width"],
           "atr": ["atr"],
           "volume": ["volume_ratio", "cvd", "cvd_slope"],
           "adx": ["adx", "adx_slope"],
           "aroon": ["aroon_oscillator"],
           "consecutive": ["consecutive_up", "consecutive_down"],
           "trend": ["1h_trend", "4h_trend", "trend_alignment", "trend_strength"]
       }
       
       self.optional_indicators = {
           # ì„ íƒì  ì§€í‘œ
           "sma": ["sma_20", "close_vs_sma_20"],
           "ema": ["ema_20", "ema_50", "ema_20_slope"],
           "stoch": ["stoch_k", "stoch_d"],
           "williams": ["williams_r"],
           "mfi": ["mfi"],
           "vwap": ["vwap"],
           "volatility": ["volatility_20"]
       }
       
       # ì „ì²´ ì§€í‘œ í†µí•©
       self.indicator_mapping = {**self.essential_indicators, **self.optional_indicators}
       
       print(f"âœ… ModelTrainer ì´ˆê¸°í™” ì™„ë£Œ: {symbol}")
   
   def start_training(self, 
                     selected_indicators: Dict[str, bool],
                     training_params: Dict,
                     progress_callback: Optional[Callable] = None) -> bool:
       """ë¹„ë™ê¸° í•™ìŠµ ì‹œì‘"""
       if self.is_training:
           print("âŒ ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
           return False
       
       try:
           # í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”
           self.training_status = {
               "status": "running",
               "current_epoch": 0,
               "total_epochs": training_params.get("epochs", 100),
               "current_batch": 0,
               "total_batches": 0,
               "metrics": {"loss": 0.0, "accuracy": 0.0, "val_loss": 0.0, "val_accuracy": 0.0},
               "start_time": datetime.now(),
               "progress_callback": progress_callback
           }
           
           # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í•™ìŠµ ì‹¤í–‰
           self.training_thread = threading.Thread(
               target=self._train_model_async,
               args=(selected_indicators, training_params),
               daemon=True
           )
           
           self.is_training = True
           self.training_thread.start()
           
           print("ğŸš€ AI ëª¨ë¸ í•™ìŠµì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.")
           return True
           
       except Exception as e:
           print(f"âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
           self.training_status["status"] = "failed"
           return False
   
   def stop_training(self) -> bool:
       """í•™ìŠµ ì¤‘ì§€"""
       if not self.is_training:
           return False
       
       try:
           self.is_training = False
           self.training_status["status"] = "stopped"
           
           if self.training_thread and self.training_thread.is_alive():
               # ìŠ¤ë ˆë“œê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
               self.training_thread.join(timeout=5)
           
           print("â¹ï¸ AI ëª¨ë¸ í•™ìŠµì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
           return True
           
       except Exception as e:
           print(f"âŒ í•™ìŠµ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
           return False
   
   def get_training_status(self) -> Dict:
       """í˜„ì¬ í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
       return self.training_status.copy()
   
   def _train_model_async(self, selected_indicators: Dict[str, bool], training_params: Dict):
       """ë¹„ë™ê¸° í•™ìŠµ ì‹¤í–‰ (ë‚´ë¶€ ë©”ì„œë“œ)"""
       try:
           print("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
           self._update_progress_callback("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
           
           # âœ… ìŠ¤ë ˆë“œ ë‚´ë¶€ì—ì„œ DataCollector ìƒì„± (SQLite ìŠ¤ë ˆë“œ ë¬¸ì œ í•´ê²°)
           from .data_collector import DataCollector
           data_collector = DataCollector(self.symbol)
           
           # 1. ë°ì´í„° ìˆ˜ì§‘
           training_days = training_params.get("training_days", 365)
           interval = training_params.get("interval", "15")
           
           # âœ… data_collector ì‚¬ìš© (self.data_collectorê°€ ì•„ë‹˜)
           df = data_collector.collect_historical_data(interval=interval, days=training_days)
           if df is None or len(df) < 1000:
               raise Exception("ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
           
           # 2. ê¸°ìˆ ì  ì§€í‘œëŠ” ì´ë¯¸ ê³„ì‚°ë¨ (data_collectorì—ì„œ)
           print("ğŸ“‹ ì„ íƒëœ ì§€í‘œ ì¶”ì¶œ ì¤‘...")
           self._update_progress_callback("ì„ íƒëœ ì§€í‘œ ì¶”ì¶œ ì¤‘...")
           feature_data = self._prepare_features(df, selected_indicators)
           
           # 3. ë¼ë²¨ë§ (ì¶”ì„¸ ì „í™˜ ê°ì§€)
           print("ğŸ·ï¸ ë¼ë²¨ ìƒì„± ì¤‘...")
           labels = self._create_labels(df)
           
           # 4. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
           print("ğŸ“ˆ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
           X, y, scaler = self._prepare_training_data(feature_data, labels, training_params)
           
           # 5. ëª¨ë¸ êµ¬ì„±
           print("ğŸ§  ëª¨ë¸ êµ¬ì„± ì¤‘...")
           model = self._build_model(X.shape, training_params)
           
           # 6. í•™ìŠµ ì‹¤í–‰
           print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
           self._update_progress_callback("ëª¨ë¸ í•™ìŠµ ì¤‘...")
           
           history = self._train_with_progress(model, X, y, training_params)
           
           # 7. ëª¨ë¸ í‰ê°€
           print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
           accuracy = self._evaluate_model(model, X, y)
           
           # 8. ëª¨ë¸ ì €ì¥ (ModelManager ì—†ì´ ì§ì ‘ ì €ì¥)
           print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
           
           # ëª¨ë¸ëª… ìƒì„±
           timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
           model_name = f"model_{timestamp}"
           
           # ëª¨ë¸ íŒŒì¼ ì €ì¥
           model_path = Path("models") / f"{model_name}.h5"
           scaler_path = Path("models") / f"{model_name}_scaler.pkl"
           info_path = Path("models") / f"{model_name}_info.json"
           
           model_path.parent.mkdir(exist_ok=True)
           model.save(str(model_path))
           
           with open(scaler_path, 'wb') as f:
               pickle.dump(scaler, f)
           
           # í•™ìŠµ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (NASê°€ ë‚˜ì¤‘ì— ì½ì„ ìˆ˜ ìˆë„ë¡)
           training_info = {
               "model_name": model_name,
               "accuracy": float(accuracy),
               "parameters": training_params,
               "indicators": selected_indicators,
               "training_duration": (datetime.now() - self.training_status["start_time"]).total_seconds(),
               "feature_columns": list(feature_data.columns),
               "data_shape": [int(x) for x in X.shape],
               "created_at": datetime.now().isoformat(),
               "symbol": self.symbol
           }
           
           with open(info_path, 'w') as f:
               json.dump(training_info, f, indent=2)
           
           # í•™ìŠµ ì™„ë£Œ
           self.training_status["status"] = "completed"
           self.training_status["metrics"]["accuracy"] = accuracy
           
           print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {accuracy:.3f}")
           print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {model_path}")
           self._update_progress_callback(f"í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {accuracy:.1%}")
           
       except Exception as e:
           print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
           self.training_status["status"] = "failed"
           self._update_progress_callback(f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
       
       finally:
           self.is_training = False
   
   def _prepare_features(self, df: pd.DataFrame, selected_indicators: Dict[str, bool]) -> pd.DataFrame:
       """ì„ íƒëœ ì§€í‘œë§Œ ì¶”ì¶œí•˜ì—¬ íŠ¹ì„± ë°ì´í„° ìƒì„±"""
       feature_columns = []
       
       print("ğŸ“‹ ì„ íƒëœ ì§€í‘œ:")
       
       # í•„ìˆ˜ ì§€í‘œëŠ” í•­ìƒ í¬í•¨
       for indicator, columns in self.essential_indicators.items():
           existing_columns = [col for col in columns if col in df.columns]
           feature_columns.extend(existing_columns)
           print(f"   âœ… {indicator}: {len(existing_columns)}ê°œ ì»¬ëŸ¼ (í•„ìˆ˜)")
       
       # ì„ íƒì  ì§€í‘œëŠ” selected_indicatorsì— ë”°ë¼
       for indicator, selected in selected_indicators.items():
           if selected and indicator in self.optional_indicators:
               columns = self.optional_indicators[indicator]
               existing_columns = [col for col in columns if col in df.columns]
               feature_columns.extend(existing_columns)
               print(f"   âœ… {indicator}: {len(existing_columns)}ê°œ ì»¬ëŸ¼ (ì„ íƒ)")
           elif selected and indicator not in self.essential_indicators:
               print(f"   âš ï¸ {indicator}: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€í‘œ")
       
       if not feature_columns:
           raise Exception("ì„ íƒëœ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì§€í‘œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
       
       print(f"ğŸ“Š ì´ íŠ¹ì„± ê°œìˆ˜: {len(feature_columns)}ê°œ")
       
       # ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜
       feature_columns = list(set(feature_columns))
       return df[feature_columns].dropna()
   
   def _create_labels(self, df: pd.DataFrame, 
                     future_window: int = 4,  # 1ì‹œê°„ í›„ (15ë¶„ë´‰ ê¸°ì¤€)
                     significant_change: float = 0.005) -> pd.Series:
       """ğŸ†• ê°œì„ ëœ ë¼ë²¨ë§ - ì›ì›¨ì´ ì¥ í•„í„°ë§ ê°•í™”"""
       
       # ë¯¸ë˜ ê°€ê²© ë³€í™”
       future_prices = df['close'].shift(-future_window)
       price_change = (future_prices - df['close']) / df['close']
       
       # MACD ê¸°ë°˜ ì¶”ì„¸ ì „í™˜
       macd_direction = df['macd'] > df['macd_signal']
       macd_change = macd_direction.diff()
       
       # ğŸ†• ì›ì›¨ì´ ì¥ ê°ì§€
       is_one_way = (
           (df['consecutive_up'] > 8) |  # ì—°ì† 8ê°œ ì´ìƒ ìƒìŠ¹
           (df['consecutive_down'] > 8) |  # ì—°ì† 8ê°œ ì´ìƒ í•˜ë½
           (df['adx'] > 40)  # ë§¤ìš° ê°•í•œ ì¶”ì„¸
       )
       
       # ğŸ†• ì¶”ì„¸ ì „í™˜ ì¡°ê±´ (ì›ì›¨ì´ ì¥ ì œì™¸)
       trend_reversal = (
           (abs(macd_change) > 0) &  # MACD ì‹ í˜¸ ì „í™˜
           (abs(price_change) > significant_change) &  # ìœ ì˜ë¯¸í•œ ê°€ê²© ë³€í™”
           (df['rsi_14'] > 30) & (df['rsi_14'] < 70) &  # RSI ì¤‘ë¦½
           (df['adx'] < 35) &  # ë„ˆë¬´ ê°•í•œ ì¶”ì„¸ ì•„ë‹˜
           (~is_one_way)  # ì›ì›¨ì´ ì¥ ì•„ë‹˜
       )
       
       # ì¶”ê°€ ì¡°ê±´: ë³¼ë¦°ì € ë°´ë“œì™€ ê±°ë˜ëŸ‰
       if 'bb_position' in df.columns and 'volume_ratio' in df.columns:
           trend_reversal = trend_reversal & (
               (df['bb_position'] > 0.2) & (df['bb_position'] < 0.8) &  # BB ì¤‘ë¦½
               (df['volume_ratio'] > 0.8)  # ì¼ì • ê±°ë˜ëŸ‰ ì´ìƒ
           )
       
       labels = trend_reversal.astype(int)
       
       positive_ratio = labels.sum() / len(labels) if len(labels) > 0 else 0
       print(f"ğŸ·ï¸ ë¼ë²¨ ë¶„í¬: ë§¤ë§¤ í—ˆìš© {positive_ratio:.1%}, ë§¤ë§¤ ê¸ˆì§€ {1-positive_ratio:.1%}")
       
       return labels
   
   def _prepare_training_data(self, feature_data: pd.DataFrame, labels: pd.Series, 
                             training_params: Dict) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:
       """í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë³€í™˜)"""
       
       sequence_length = training_params.get("sequence_length", 60)
       
       # ë°ì´í„° ì •ê·œí™”
       scaler = MinMaxScaler()
       scaled_features = scaler.fit_transform(feature_data)
       
       # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
       X, y = [], []
       
       for i in range(sequence_length, len(scaled_features)):
           if not self.is_training:  # ì¤‘ì§€ ìš”ì²­ í™•ì¸
               break
               
           X.append(scaled_features[i-sequence_length:i])
           y.append(labels.iloc[i])
       
       X = np.array(X)
       y = np.array(y)
       
       print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í˜•íƒœ: X={X.shape}, y={y.shape}")
       print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}")
       print(f"   - íŠ¹ì„± ê°œìˆ˜: {X.shape[2] if len(X.shape) > 2 else 0}")
       print(f"   - ìƒ˜í”Œ ê°œìˆ˜: {X.shape[0]}")
       
       return X, y, scaler
   
   def _build_model(self, input_shape: Tuple, training_params: Dict) -> tf.keras.Model:
       """LSTM ëª¨ë¸ êµ¬ì„±"""
       
       model = Sequential([
           # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´
           LSTM(64, return_sequences=True, input_shape=input_shape[1:]),
           Dropout(0.2),
           BatchNormalization(),
           
           # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´  
           LSTM(32, return_sequences=False),
           Dropout(0.2),
           BatchNormalization(),
           
           # Dense ë ˆì´ì–´ë“¤
           Dense(32, activation='relu'),
           Dropout(0.3),
           Dense(16, activation='relu'),
           Dropout(0.2),
           
           # ì¶œë ¥ ë ˆì´ì–´ (ì´ì§„ ë¶„ë¥˜)
           Dense(1, activation='sigmoid')
       ])
       
       # ì»´íŒŒì¼
       learning_rate = training_params.get("learning_rate", 0.001)
       optimizer = Adam(learning_rate=learning_rate)
       
       model.compile(
           optimizer=optimizer,
           loss='binary_crossentropy',
           metrics=['accuracy']
       )
       
       print("ğŸ§  ëª¨ë¸ êµ¬ì¡°:")
       model.summary()
       
       return model
   
   def _train_with_progress(self, model: tf.keras.Model, X: np.ndarray, y: np.ndarray, 
                          training_params: Dict) -> tf.keras.callbacks.History:
       """ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ í•™ìŠµ ì‹¤í–‰"""
       
       # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
       validation_split = training_params.get("validation_split", 20) / 100
       X_train, X_val, y_train, y_val = train_test_split(
           X, y, test_size=validation_split, random_state=42, stratify=y
       )
       
       # ë°°ì¹˜ í¬ê¸°
       batch_size = training_params.get("batch_size", 32)
       epochs = training_params.get("epochs", 100)
       
       # ì´ ë°°ì¹˜ ìˆ˜ ê³„ì‚°
       total_batches = len(X_train) // batch_size
       self.training_status["total_batches"] = total_batches
       
       # ì½œë°± í•¨ìˆ˜ë“¤
       callbacks = [
           EarlyStopping(patience=10, restore_best_weights=True),
           ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7),
           TrainingProgressCallback(self)  # ì»¤ìŠ¤í…€ ì½œë°±
       ]
       
       # í•™ìŠµ ì‹¤í–‰
       history = model.fit(
           X_train, y_train,
           batch_size=batch_size,
           epochs=epochs,
           validation_data=(X_val, y_val),
           callbacks=callbacks,
           verbose=0  # ì§„í–‰ë¥ ì€ ì»¤ìŠ¤í…€ ì½œë°±ì—ì„œ ì²˜ë¦¬
       )
       
       return history
   
   def _evaluate_model(self, model: tf.keras.Model, X: np.ndarray, y: np.ndarray) -> float:
       """ëª¨ë¸ í‰ê°€"""
       
       # ì˜ˆì¸¡ ìˆ˜í–‰
       y_pred_proba = model.predict(X, verbose=0)
       y_pred = (y_pred_proba > 0.5).astype(int).flatten()
       
       # ì •í™•ë„ ê³„ì‚°
       accuracy = accuracy_score(y, y_pred)
       
       # ìƒì„¸ ë¦¬í¬íŠ¸
       print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
       print(f"   ì •í™•ë„: {accuracy:.3f}")
       print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
       print(classification_report(y, y_pred, target_names=['ë§¤ë§¤ ê¸ˆì§€', 'ë§¤ë§¤ í—ˆìš©']))
       
       return accuracy
   
   def _update_progress_callback(self, message: str):
       """ì§„í–‰ë¥  ì½œë°± ì—…ë°ì´íŠ¸"""
       if self.training_status.get("progress_callback"):
           try:
               self.training_status["progress_callback"](message)
           except:
               pass

class TrainingProgressCallback(tf.keras.callbacks.Callback):
   """í•™ìŠµ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì½œë°±"""
   
   def __init__(self, trainer: ModelTrainer):
       super().__init__()
       self.trainer = trainer
   
   def on_epoch_begin(self, epoch, logs=None):
       if not self.trainer.is_training:
           self.model.stop_training = True
           return
           
       self.trainer.training_status["current_epoch"] = epoch + 1
       self.trainer._update_progress_callback(f"ì—í­ {epoch + 1}/{self.trainer.training_status['total_epochs']}")
   
   def on_batch_end(self, batch, logs=None):
       if not self.trainer.is_training:
           self.model.stop_training = True
           return
           
       self.trainer.training_status["current_batch"] = batch + 1
       
       # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
       if logs:
           metrics = self.trainer.training_status["metrics"]
           metrics["loss"] = logs.get("loss", 0)
           metrics["accuracy"] = logs.get("accuracy", 0)
           metrics["val_loss"] = logs.get("val_loss", 0)
           metrics["val_accuracy"] = logs.get("val_accuracy", 0)
   
   def on_epoch_end(self, epoch, logs=None):
       if logs:
           metrics = self.trainer.training_status["metrics"] 
           metrics["loss"] = logs.get("loss", 0)
           metrics["accuracy"] = logs.get("accuracy", 0)
           metrics["val_loss"] = logs.get("val_loss", 0)
           metrics["val_accuracy"] = logs.get("val_accuracy", 0)
           
           print(f"ì—í­ {epoch + 1}: ì†ì‹¤={metrics['loss']:.4f}, ì •í™•ë„={metrics['accuracy']:.3f}")

# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
# ============================================================================

if __name__ == "__main__":
   print("ğŸš€ ModelTrainer í…ŒìŠ¤íŠ¸ ì‹œì‘")
   
   # í•™ìŠµê¸° ìƒì„±
   trainer = ModelTrainer("BTCUSDT")
   
   # ì„ íƒëœ ì§€í‘œ (ì›¹ UIì—ì„œ ë°›ì„ ë°ì´í„°)
   selected_indicators = {
       # ì„ íƒì  ì§€í‘œë§Œ (í•„ìˆ˜ ì§€í‘œëŠ” ìë™ í¬í•¨)
       "sma": False,
       "ema": False,
       "stoch": False,
       "williams": False,
       "mfi": True,
       "vwap": True,
       "volatility": True
   }
   
   # í•™ìŠµ íŒŒë¼ë¯¸í„° (ì›¹ UIì—ì„œ ë°›ì„ ë°ì´í„°)
   training_params = {
       "training_days": 30,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 30ì¼ë§Œ
       "epochs": 10,         # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ì—í­ë§Œ
       "batch_size": 32,
       "learning_rate": 0.001,
       "sequence_length": 60,
       "validation_split": 20,
       "interval": "15"
   }
   
   # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
   def progress_callback(message):
       print(f"ğŸ“¢ {message}")
   
   # í•™ìŠµ ì‹œì‘
   success = trainer.start_training(selected_indicators, training_params, progress_callback)
   
   if success:
       print("âœ… í•™ìŠµì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
       
       # ìƒíƒœ ëª¨ë‹ˆí„°ë§
       while trainer.is_training:
           status = trainer.get_training_status()
           print(f"ì§„í–‰ë¥ : {status['current_epoch']}/{status['total_epochs']} ì—í­")
           time.sleep(5)
       
       # ìµœì¢… ê²°ê³¼
       final_status = trainer.get_training_status()
       print(f"ğŸ í•™ìŠµ ì™„ë£Œ! ìƒíƒœ: {final_status['status']}")
       if final_status['status'] == 'completed':
           print(f"   ìµœì¢… ì •í™•ë„: {final_status['metrics']['accuracy']:.3f}")
       
   else:
       print("âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨")
   
   print("\nâœ… ModelTrainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ")