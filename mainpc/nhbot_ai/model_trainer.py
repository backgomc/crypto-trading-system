# íŒŒì¼ ê²½ë¡œ: mainpc/nhbot_ai/model_trainer.py
# ì½”ë“œëª…: AI ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (3-í´ë˜ìŠ¤ ë¶„ë¥˜, MACD ì œì™¸ ë²„ì „)

import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import pickle
import json
from datetime import datetime
from pathlib import Path
import threading
import time
from typing import Dict, List, Optional, Tuple, Callable

class ModelTrainer:
    """AI ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (3-í´ë˜ìŠ¤: none/long/short)"""
    
    def __init__(self, symbol: str = "BTCUSDT"):
        self.symbol = symbol
        
        # í•™ìŠµ ìƒíƒœ ê´€ë¦¬
        self.is_training = False
        self.training_thread = None
        self.training_status = {
            "status": "idle",  # idle, running, completed, failed
            "current_epoch": 0,
            "total_epochs": 0,
            "current_batch": 0,
            "total_batches": 0,
            "metrics": {
                "loss": 0.0,
                "accuracy": 0.0,
                "val_loss": 0.0,
                "val_accuracy": 0.0
            },
            "start_time": None,
            "progress_callback": None
        }
        
        # ğŸ†• MACD ì œì™¸í•œ í•„ìˆ˜ ì§€í‘œ
        self.essential_indicators = {
            # ê°€ê²© ì›€ì§ì„
            "price": ["close", "price_change", "price_change_abs", "hl_range"],
            
            # ëª¨ë©˜í…€ (MACD ì™„ì „ ì œì™¸!)
            "rsi": ["rsi_14", "rsi_30"],
            "stoch": ["stoch_k", "stoch_d"],
            "williams": ["williams_r"],
            
            # ì¶”ì„¸ ê°•ë„
            "adx": ["adx", "adx_slope"],
            "aroon": ["aroon_up", "aroon_down", "aroon_oscillator"],
            
            # ë³€ë™ì„±
            "bb": ["bb_position", "bb_width"],
            "atr": ["atr"],
            
            # ê±°ë˜ëŸ‰
            "volume": ["volume_ratio", "cvd", "cvd_slope", "mfi"],
            
            # íŒ¨í„´
            "consecutive": ["consecutive_up", "consecutive_down"],
            "trend": ["1h_trend", "4h_trend", "trend_alignment", "trend_strength"]
        }
        
        self.optional_indicators = {
            # ì„ íƒì  ì§€í‘œ
            "sma": ["sma_20", "close_vs_sma_20", "sma_20_slope"],
            "ema": ["ema_20", "ema_50", "ema_20_slope"],
            "volatility": ["volatility_10", "volatility_20"],
            "vwap": ["vwap"],
            "pivot": ["pivot_position"],
            "zscore": ["zscore_20", "zscore_50"]
        }
        
        # ì „ì²´ ì§€í‘œ í†µí•©
        self.indicator_mapping = {**self.essential_indicators, **self.optional_indicators}
        
        print(f"âœ… ModelTrainer ì´ˆê¸°í™” ì™„ë£Œ: {symbol}")
        print(f"   ëª¨ë“œ: 3-í´ë˜ìŠ¤ ë¶„ë¥˜ (none/long/short)")
    
    def start_training(self, 
                      selected_indicators: Dict[str, bool],
                      training_params: Dict,
                      progress_callback: Optional[Callable] = None) -> bool:
        """ë¹„ë™ê¸° í•™ìŠµ ì‹œì‘"""
        if self.is_training:
            print("âŒ ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return False
        
        try:
            # í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”
            self.training_status = {
                "status": "running",
                "current_epoch": 0,
                "total_epochs": training_params.get("epochs", 100),
                "current_batch": 0,
                "total_batches": 0,
                "metrics": {"loss": 0.0, "accuracy": 0.0, "val_loss": 0.0, "val_accuracy": 0.0},
                "start_time": datetime.now(),
                "progress_callback": progress_callback
            }
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í•™ìŠµ ì‹¤í–‰
            self.training_thread = threading.Thread(
                target=self._train_model_async,
                args=(selected_indicators, training_params),
                daemon=True
            )
            
            self.is_training = True
            self.training_thread.start()
            
            print("ğŸš€ AI ëª¨ë¸ í•™ìŠµì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
            self.training_status["status"] = "failed"
            return False
    
    def stop_training(self) -> bool:
        """í•™ìŠµ ì¤‘ì§€"""
        if not self.is_training:
            return False
        
        try:
            self.is_training = False
            self.training_status["status"] = "stopped"
            
            if self.training_thread and self.training_thread.is_alive():
                self.training_thread.join(timeout=5)
            
            print("â¹ï¸ AI ëª¨ë¸ í•™ìŠµì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_training_status(self) -> Dict:
        """í˜„ì¬ í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
        return self.training_status.copy()
    
    def _train_model_async(self, selected_indicators: Dict[str, bool], training_params: Dict):
        """ë¹„ë™ê¸° í•™ìŠµ ì‹¤í–‰ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            print("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
            self._update_progress_callback("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # ìŠ¤ë ˆë“œ ë‚´ë¶€ì—ì„œ DataCollector ìƒì„±
            from .data_collector import DataCollector
            data_collector = DataCollector(self.symbol)
            
            # 1. ë°ì´í„° ìˆ˜ì§‘
            training_days = training_params.get("training_days", 365)
            interval = training_params.get("interval", "15")
            
            df = data_collector.collect_historical_data(interval=interval, days=training_days)
            if df is None or len(df) < 1000:
                raise Exception("ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2. ì„ íƒëœ ì§€í‘œ ì¶”ì¶œ (MACD ì œì™¸)
            print("ğŸ“‹ ì„ íƒëœ ì§€í‘œ ì¶”ì¶œ ì¤‘...")
            self._update_progress_callback("ì„ íƒëœ ì§€í‘œ ì¶”ì¶œ ì¤‘...")
            feature_data = self._prepare_features(df, selected_indicators)
            
            # 3. ğŸ†• 3-í´ë˜ìŠ¤ ë¼ë²¨ë§ (ì¶”ì„¸ ì „í™˜ ë°©í–¥ êµ¬ë¶„)
            print("ğŸ·ï¸ ë¼ë²¨ ìƒì„± ì¤‘ (3-í´ë˜ìŠ¤)...")
            labels = self._create_labels_3class(df)
            
            # 4. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            print("ğŸ“ˆ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            X, y, scaler = self._prepare_training_data(feature_data, labels, training_params)
            
            # 5. ğŸ†• 3-í´ë˜ìŠ¤ ëª¨ë¸ êµ¬ì„±
            print("ğŸ§  ëª¨ë¸ êµ¬ì„± ì¤‘...")
            model = self._build_model_3class(X.shape, training_params)
            
            # 6. í•™ìŠµ ì‹¤í–‰
            print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            self._update_progress_callback("ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            history = self._train_with_progress(model, X, y, training_params)
            
            # 7. ëª¨ë¸ í‰ê°€
            print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
            accuracy = self._evaluate_model_3class(model, X, y)
            
            # 8. ëª¨ë¸ ì €ì¥
            print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = f"model_3class_{timestamp}"
            
            model_path = Path("models") / f"{model_name}.h5"
            scaler_path = Path("models") / f"{model_name}_scaler.pkl"
            info_path = Path("models") / f"{model_name}_info.json"
            
            model_path.parent.mkdir(exist_ok=True)
            model.save(str(model_path))
            
            with open(scaler_path, 'wb') as f:
                pickle.dump(scaler, f)
            
            # í•™ìŠµ ì •ë³´ ì €ì¥
            training_info = {
                "model_name": model_name,
                "model_type": "3-class",  # ëª¨ë¸ íƒ€ì… ëª…ì‹œ
                "classes": ["none", "long", "short"],
                "accuracy": float(accuracy),
                "parameters": training_params,
                "indicators": selected_indicators,
                "training_duration": (datetime.now() - self.training_status["start_time"]).total_seconds(),
                "feature_columns": list(feature_data.columns),
                "data_shape": [int(x) for x in X.shape],
                "created_at": datetime.now().isoformat(),
                "symbol": self.symbol
            }
            
            with open(info_path, 'w') as f:
                json.dump(training_info, f, indent=2)
            
            # í•™ìŠµ ì™„ë£Œ
            self.training_status["status"] = "completed"
            self.training_status["metrics"]["accuracy"] = accuracy
            
            print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {accuracy:.3f}")
            print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {model_path}")
            self._update_progress_callback(f"í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {accuracy:.1%}")
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            self.training_status["status"] = "failed"
            self._update_progress_callback(f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        
        finally:
            self.is_training = False
    
    def _prepare_features(self, df: pd.DataFrame, selected_indicators: Dict[str, bool]) -> pd.DataFrame:
        """ì„ íƒëœ ì§€í‘œë§Œ ì¶”ì¶œ (MACD ì™„ì „ ì œì™¸)"""
        feature_columns = []
        
        # MACD ê´€ë ¨ ì»¬ëŸ¼ ì œì™¸ ë¦¬ìŠ¤íŠ¸
        macd_columns = ['macd', 'macd_signal', 'macd_histogram']
        
        print("ğŸ“‹ ì„ íƒëœ ì§€í‘œ (MACD ì œì™¸):")
        
        # í•„ìˆ˜ ì§€í‘œëŠ” í•­ìƒ í¬í•¨
        for indicator, columns in self.essential_indicators.items():
            # MACD ì»¬ëŸ¼ ì œì™¸
            existing_columns = [
                col for col in columns 
                if col in df.columns and col not in macd_columns
            ]
            feature_columns.extend(existing_columns)
            print(f"   âœ… {indicator}: {len(existing_columns)}ê°œ ì»¬ëŸ¼ (í•„ìˆ˜)")
        
        # ì„ íƒì  ì§€í‘œ
        for indicator, selected in selected_indicators.items():
            if selected and indicator in self.optional_indicators:
                columns = self.optional_indicators[indicator]
                existing_columns = [
                    col for col in columns 
                    if col in df.columns and col not in macd_columns
                ]
                feature_columns.extend(existing_columns)
                print(f"   âœ… {indicator}: {len(existing_columns)}ê°œ ì»¬ëŸ¼ (ì„ íƒ)")
        
        if not feature_columns:
            raise Exception("ì„ íƒëœ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¤‘ë³µ ì œê±°
        feature_columns = list(set(feature_columns))
        
        # MACD ì»¬ëŸ¼ì´ ì‹¤ìˆ˜ë¡œ í¬í•¨ë˜ì—ˆëŠ”ì§€ ìµœì¢… ì²´í¬
        feature_columns = [col for col in feature_columns if col not in macd_columns]
        
        print(f"ğŸ“Š ì´ íŠ¹ì„± ê°œìˆ˜: {len(feature_columns)}ê°œ")
        print(f"   (MACD ê´€ë ¨ ì§€í‘œ ì œì™¸ í™•ì¸ ì™„ë£Œ)")
        
        return df[feature_columns].dropna()
    
    def _create_labels_3class(self, df: pd.DataFrame) -> pd.Series:
        """ğŸ†• 3-í´ë˜ìŠ¤ ë¼ë²¨ë§ (0: none, 1: long, 2: short)"""
        
        labels = pd.Series(0, index=df.index)  # ê¸°ë³¸ê°’: none
        
        # ê°€ê²© ê¸°ë°˜ ì¶”ì„¸ ì „í™˜ ê°ì§€
        price_change_1h = df['close'].pct_change(periods=4)  # 1ì‹œê°„ ë³€í™”ìœ¨ (15ë¶„ë´‰ 4ê°œ)
        price_change_30m = df['close'].pct_change(periods=2)  # 30ë¶„ ë³€í™”ìœ¨
        
        # RSI ê¸°ë°˜ ì‹ í˜¸
        rsi = df['rsi_14'] if 'rsi_14' in df.columns else pd.Series(50, index=df.index)
        
        # ADX ê¸°ë°˜ ì¶”ì„¸ ê°•ë„
        adx = df['adx'] if 'adx' in df.columns else pd.Series(25, index=df.index)
        
        # Aroon ê¸°ë°˜ ì¶”ì„¸ ì „í™˜
        aroon_osc = df['aroon_oscillator'] if 'aroon_oscillator' in df.columns else pd.Series(0, index=df.index)
        
        # ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜
        bb_pos = df['bb_position'] if 'bb_position' in df.columns else pd.Series(0.5, index=df.index)
        
        # ì—°ì† ìƒìŠ¹/í•˜ë½
        consec_up = df['consecutive_up'] if 'consecutive_up' in df.columns else pd.Series(0, index=df.index)
        consec_down = df['consecutive_down'] if 'consecutive_down' in df.columns else pd.Series(0, index=df.index)
        
        # ğŸ”´ ìƒìŠ¹ ì „í™˜ ì‹ í˜¸ (í•˜ë½â†’ìƒìŠ¹) - í´ë˜ìŠ¤ 1: long
        long_signals = (
            # ì¡°ê±´ 1: ê°€ê²© ë°˜ì „ (í•˜ë½ í›„ ìƒìŠ¹)
            ((price_change_30m.shift(1) < -0.001) & (price_change_30m > 0.001)) |
            
            # ì¡°ê±´ 2: RSI ê³¼ë§¤ë„ íƒˆì¶œ
            ((rsi.shift(1) < 35) & (rsi > 35) & (rsi < 50)) |
            
            # ì¡°ê±´ 3: ë³¼ë¦°ì € ë°´ë“œ í•˜ë‹¨ ë°˜ë“±
            ((bb_pos.shift(1) < 0.1) & (bb_pos > 0.15)) |
            
            # ì¡°ê±´ 4: Aroon ìƒìŠ¹ ì „í™˜
            ((aroon_osc.shift(1) < -30) & (aroon_osc > -10)) |
            
            # ì¡°ê±´ 5: ì—°ì† í•˜ë½ í›„ ë°˜ì „
            ((consec_down > 5) & (price_change_30m > 0.001))
        )
        
        # ğŸ”µ í•˜ë½ ì „í™˜ ì‹ í˜¸ (ìƒìŠ¹â†’í•˜ë½) - í´ë˜ìŠ¤ 2: short
        short_signals = (
            # ì¡°ê±´ 1: ê°€ê²© ë°˜ì „ (ìƒìŠ¹ í›„ í•˜ë½)
            ((price_change_30m.shift(1) > 0.001) & (price_change_30m < -0.001)) |
            
            # ì¡°ê±´ 2: RSI ê³¼ë§¤ìˆ˜ íƒˆì¶œ
            ((rsi.shift(1) > 65) & (rsi < 65) & (rsi > 50)) |
            
            # ì¡°ê±´ 3: ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨ ë°˜ë½
            ((bb_pos.shift(1) > 0.9) & (bb_pos < 0.85)) |
            
            # ì¡°ê±´ 4: Aroon í•˜ë½ ì „í™˜
            ((aroon_osc.shift(1) > 30) & (aroon_osc < 10)) |
            
            # ì¡°ê±´ 5: ì—°ì† ìƒìŠ¹ í›„ ë°˜ì „
            ((consec_up > 5) & (price_change_30m < -0.001))
        )
        
        # ì›ì›¨ì´ ì¥ í•„í„° (ë„ˆë¬´ ê°•í•œ ì¶”ì„¸ëŠ” ì œì™¸)
        extreme_trend = (adx > 50) | (consec_up > 10) | (consec_down > 10)
        
        # ìµœì†Œ ê±°ë˜ëŸ‰ í•„í„°
        if 'volume_ratio' in df.columns:
            sufficient_volume = df['volume_ratio'] > 0.5
        else:
            sufficient_volume = True
        
        # ë¼ë²¨ í• ë‹¹
        labels[long_signals & ~extreme_trend & sufficient_volume] = 1  # long
        labels[short_signals & ~extreme_trend & sufficient_volume] = 2  # short
        
        # ì¤‘ë³µ ì‹ í˜¸ ì²˜ë¦¬ (ë™ì‹œì— longê³¼ shortê°€ ë‚˜ì˜¤ë©´ noneìœ¼ë¡œ)
        conflicting = long_signals & short_signals
        labels[conflicting] = 0
        
        # í†µê³„ ì¶œë ¥
        none_count = (labels == 0).sum()
        long_count = (labels == 1).sum()
        short_count = (labels == 2).sum()
        total_count = len(labels)
        
        print(f"ğŸ·ï¸ 3-í´ë˜ìŠ¤ ë¼ë²¨ ë¶„í¬:")
        print(f"   - None (í´ë˜ìŠ¤ 0): {none_count:,}ê°œ ({none_count/total_count*100:.1f}%)")
        print(f"   - Long (í´ë˜ìŠ¤ 1): {long_count:,}ê°œ ({long_count/total_count*100:.1f}%)")
        print(f"   - Short (í´ë˜ìŠ¤ 2): {short_count:,}ê°œ ({short_count/total_count*100:.1f}%)")
        print(f"   - ì´ ì¶”ì„¸ ì „í™˜: {long_count + short_count:,}ê°œ ({(long_count + short_count)/total_count*100:.1f}%)")
        
        # ëª©í‘œ: ì „ì²´ì˜ 3~5% ì •ë„ê°€ ì¶”ì„¸ ì „í™˜ ì‹ í˜¸
        signal_ratio = (long_count + short_count) / total_count
        if signal_ratio < 0.02:
            print("âš ï¸ ì¶”ì„¸ ì „í™˜ ì‹ í˜¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (2% ë¯¸ë§Œ)")
        elif signal_ratio > 0.10:
            print("âš ï¸ ì¶”ì„¸ ì „í™˜ ì‹ í˜¸ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ (10% ì´ˆê³¼)")
        else:
            print("âœ… ì ì ˆí•œ ì‹ í˜¸ ë¹„ìœ¨ì…ë‹ˆë‹¤")
        
        return labels
    
    def _prepare_training_data(self, feature_data: pd.DataFrame, labels: pd.Series, 
                              training_params: Dict) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:
        """í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë³€í™˜)"""
        
        sequence_length = training_params.get("sequence_length", 60)
        
        # ë°ì´í„° ì •ê·œí™”
        scaler = MinMaxScaler()
        scaled_features = scaler.fit_transform(feature_data)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X, y = [], []
        
        for i in range(sequence_length, len(scaled_features)):
            if not self.is_training:
                break
                
            X.append(scaled_features[i-sequence_length:i])
            y.append(labels.iloc[i])
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í˜•íƒœ: X={X.shape}, y={y.shape}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}")
        print(f"   - íŠ¹ì„± ê°œìˆ˜: {X.shape[2] if len(X.shape) > 2 else 0}")
        print(f"   - ìƒ˜í”Œ ê°œìˆ˜: {X.shape[0]}")
        
        return X, y, scaler
    
    def _build_model_3class(self, input_shape: Tuple, training_params: Dict) -> tf.keras.Model:
        """ğŸ†• 3-í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ LSTM ëª¨ë¸"""
        
        model = Sequential([
            # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´ (ì¦ê°€)
            LSTM(128, return_sequences=True, input_shape=input_shape[1:]),
            Dropout(0.3),
            BatchNormalization(),
            
            # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(64, return_sequences=False),
            Dropout(0.3),
            BatchNormalization(),
            
            # Dense ë ˆì´ì–´ë“¤
            Dense(64, activation='relu'),
            Dropout(0.4),
            Dense(32, activation='relu'),
            Dropout(0.3),
            Dense(16, activation='relu'),
            Dropout(0.2),
            
            # ğŸ†• ì¶œë ¥ ë ˆì´ì–´ (3-í´ë˜ìŠ¤)
            Dense(3, activation='softmax')  # 3ê°œ í´ë˜ìŠ¤: none, long, short
        ])
        
        # ì»´íŒŒì¼
        learning_rate = training_params.get("learning_rate", 0.001)
        optimizer = Adam(learning_rate=learning_rate)
        
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',  # 3-í´ë˜ìŠ¤ìš© loss
            metrics=['accuracy', 
                    tf.keras.metrics.SparseCategoricalAccuracy(name='categorical_accuracy')]
        )
        
        print("ğŸ§  3-í´ë˜ìŠ¤ ëª¨ë¸ êµ¬ì¡°:")
        model.summary()
        
        return model
    
    def _train_with_progress(self, model: tf.keras.Model, X: np.ndarray, y: np.ndarray, 
                           training_params: Dict) -> tf.keras.callbacks.History:
        """ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ í•™ìŠµ ì‹¤í–‰"""
        
        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
        validation_split = training_params.get("validation_split", 20) / 100
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=validation_split, random_state=42, stratify=y
        )
        
        # ğŸ†• í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
        unique_classes = np.unique(y_train)
        class_weights = compute_class_weight(
            'balanced',
            classes=unique_classes,
            y=y_train
        )
        class_weight_dict = {int(unique_classes[i]): class_weights[i] for i in range(len(unique_classes))}
        
        print(f"âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:")
        print(f"   - None (0): {class_weight_dict.get(0, 1.0):.2f}")
        print(f"   - Long (1): {class_weight_dict.get(1, 1.0):.2f}")
        print(f"   - Short (2): {class_weight_dict.get(2, 1.0):.2f}")
        
        # ë°°ì¹˜ í¬ê¸°
        batch_size = training_params.get("batch_size", 32)
        epochs = training_params.get("epochs", 100)
        
        # ì´ ë°°ì¹˜ ìˆ˜ ê³„ì‚°
        total_batches = len(X_train) // batch_size
        self.training_status["total_batches"] = total_batches
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        callbacks = [
            EarlyStopping(
                patience=20,  # 10 â†’ 20ìœ¼ë¡œ ì¦ê°€
                restore_best_weights=True,
                monitor='val_loss',
                verbose=1
            ),
            ReduceLROnPlateau(
                factor=0.5, 
                patience=10,  # 5 â†’ 10ìœ¼ë¡œ ì¦ê°€
                min_lr=1e-7,
                verbose=1
            ),
            TrainingProgressCallback(self)
        ]
        
        # í•™ìŠµ ì‹¤í–‰ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
        history = model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(X_val, y_val),
            class_weight=class_weight_dict,  # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
            callbacks=callbacks,
            verbose=0
        )
        
        return history
    
    def _evaluate_model_3class(self, model: tf.keras.Model, X: np.ndarray, y: np.ndarray) -> float:
        """ğŸ†• 3-í´ë˜ìŠ¤ ëª¨ë¸ í‰ê°€"""
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred_proba = model.predict(X, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = accuracy_score(y, y_pred)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y, y_pred)
        
        # ìƒì„¸ ë¦¬í¬íŠ¸
        print("\nğŸ“Š 3-í´ë˜ìŠ¤ ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
        print(f"   ì „ì²´ ì •í™•ë„: {accuracy:.3f}")
        
        print("\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        report = classification_report(y, y_pred, 
                                      target_names=['None (0)', 'Long (1)', 'Short (2)'],
                                      output_dict=True)
        
        for class_name in ['None (0)', 'Long (1)', 'Short (2)']:
            if class_name in report:
                metrics = report[class_name]
                print(f"   {class_name}:")
                print(f"      - Precision: {metrics['precision']:.3f}")
                print(f"      - Recall: {metrics['recall']:.3f}")
                print(f"      - F1-Score: {metrics['f1-score']:.3f}")
        
        print("\nğŸ¯ í˜¼ë™ í–‰ë ¬:")
        print("       ì˜ˆì¸¡ â†’")
        print("ì‹¤ì œâ†“  None  Long  Short")
        labels = ['None', 'Long', 'Short']
        for i, label in enumerate(labels):
            print(f"{label:5} {cm[i][0]:5} {cm[i][1]:5} {cm[i][2]:5}")
        
        # ì¶”ì„¸ ì „í™˜ ê°ì§€ ì„±ëŠ¥ (Long + Short)
        signal_precision = (report.get('Long (1)', {}).get('precision', 0) + 
                          report.get('Short (2)', {}).get('precision', 0)) / 2
        signal_recall = (report.get('Long (1)', {}).get('recall', 0) + 
                       report.get('Short (2)', {}).get('recall', 0)) / 2
        
        print(f"\nğŸ” ì¶”ì„¸ ì „í™˜ ê°ì§€ ì„±ëŠ¥:")
        print(f"   - í‰ê·  Precision: {signal_precision:.3f}")
        print(f"   - í‰ê·  Recall: {signal_recall:.3f}")
        
        return accuracy
    
    def _update_progress_callback(self, message: str):
        """ì§„í–‰ë¥  ì½œë°± ì—…ë°ì´íŠ¸"""
        if self.training_status.get("progress_callback"):
            try:
                self.training_status["progress_callback"](message)
            except:
                pass

class TrainingProgressCallback(tf.keras.callbacks.Callback):
    """í•™ìŠµ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì½œë°±"""
    
    def __init__(self, trainer: ModelTrainer):
        super().__init__()
        self.trainer = trainer
    
    def on_epoch_begin(self, epoch, logs=None):
        if not self.trainer.is_training:
            self.model.stop_training = True
            return
            
        self.trainer.training_status["current_epoch"] = epoch + 1
        self.trainer._update_progress_callback(f"ì—í­ {epoch + 1}/{self.trainer.training_status['total_epochs']}")
    
    def on_batch_end(self, batch, logs=None):
        if not self.trainer.is_training:
            self.model.stop_training = True
            return
            
        self.trainer.training_status["current_batch"] = batch + 1
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if logs:
            metrics = self.trainer.training_status["metrics"]
            metrics["loss"] = logs.get("loss", 0)
            metrics["accuracy"] = logs.get("accuracy", 0)
            metrics["val_loss"] = logs.get("val_loss", 0)
            metrics["val_accuracy"] = logs.get("val_accuracy", 0)
    
    def on_epoch_end(self, epoch, logs=None):
        if logs:
            metrics = self.trainer.training_status["metrics"] 
            metrics["loss"] = logs.get("loss", 0)
            metrics["accuracy"] = logs.get("accuracy", 0)
            metrics["val_loss"] = logs.get("val_loss", 0)
            metrics["val_accuracy"] = logs.get("val_accuracy", 0)
            
            print(f"ì—í­ {epoch + 1}: ì†ì‹¤={metrics['loss']:.4f}, ì •í™•ë„={metrics['accuracy']:.3f}")

# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ ModelTrainer 3-í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = ModelTrainer("BTCUSDT")
    
    # ì„ íƒëœ ì§€í‘œ (MACD ì œì™¸)
    selected_indicators = {
        "sma": True,
        "ema": True,
        "stoch": True,
        "williams": True,
        "mfi": True,
        "vwap": True,
        "volatility": True,
        "pivot": True,
        "zscore": True
    }
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    training_params = {
        "training_days": 365,  # 1ë…„ ë°ì´í„°
        "epochs": 100,
        "batch_size": 32,
        "learning_rate": 0.001,
        "sequence_length": 60,
        "validation_split": 20,
        "interval": "15"
    }
    
    # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
    def progress_callback(message):
        print(f"ğŸ“¢ {message}")
    
    # í•™ìŠµ ì‹œì‘
    success = trainer.start_training(selected_indicators, training_params, progress_callback)
    
    if success:
        print("âœ… 3-í´ë˜ìŠ¤ í•™ìŠµì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   - í´ë˜ìŠ¤ 0: None (ì¶”ì„¸ ì „í™˜ ì•„ë‹˜)")
        print("   - í´ë˜ìŠ¤ 1: Long (ìƒìŠ¹ ì „í™˜)")
        print("   - í´ë˜ìŠ¤ 2: Short (í•˜ë½ ì „í™˜)")
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì˜ˆì‹œ)
        import time
        for _ in range(3):
            time.sleep(5)
            status = trainer.get_training_status()
            if status['status'] == 'running':
                print(f"   ì§„í–‰ì¤‘: {status['current_epoch']}/{status['total_epochs']} ì—í­")
    else:
        print("âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨")
    
    print("\nâœ… ModelTrainer 3-í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")