# íŒŒì¼ ê²½ë¡œ: core/ai/model_trainer.py
# ì½”ë“œëª…: AI ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (ì‚¬ìš©ì ì„ íƒ ì§€í‘œ ê¸°ë°˜)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pickle
import json
from datetime import datetime
from pathlib import Path
import threading
import time
from typing import Dict, List, Optional, Tuple, Callable

from .data_collector import DataCollector
from .model_manager import ModelManager

class ModelTrainer:
    """AI ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›)"""
    
    def __init__(self, symbol: str = "BTCUSDT"):
        self.symbol = symbol
        self.model_manager = ModelManager()
        self.data_collector = DataCollector(symbol)
        
        # í•™ìŠµ ìƒíƒœ ê´€ë¦¬
        self.is_training = False
        self.training_thread = None
        self.training_status = {
            "status": "idle",  # idle, running, completed, failed
            "current_epoch": 0,
            "total_epochs": 0,
            "current_batch": 0,
            "total_batches": 0,
            "metrics": {
                "loss": 0.0,
                "accuracy": 0.0,
                "val_loss": 0.0,
                "val_accuracy": 0.0
            },
            "start_time": None,
            "progress_callback": None
        }
        
        # ì§€í‘œ ë§¤í•‘ (ì›¹ UI ì„ íƒ â†’ ì‹¤ì œ ì»¬ëŸ¼ëª…)
        self.indicator_mapping = {
            # ê°€ê²© ì§€í‘œ
            "price": ["open", "high", "low", "close", "volume", "price_change", "hl_range"],
            "sma": ["sma_5", "sma_10", "sma_20", "sma_50", "close_vs_sma_20"],
            "ema": ["ema_5", "ema_10", "ema_20", "ema_50", "close_vs_ema_20"],
            "bb": ["bb_upper", "bb_lower", "bb_position", "bb_width"],
            
            # ëª¨ë©˜í…€ ì§€í‘œ
            "rsi": ["rsi_14", "rsi_30"],
            "macd": ["macd", "macd_signal", "macd_histogram"],
            "stoch": ["stoch_k", "stoch_d"],
            "williams": ["williams_r"],
            
            # ê±°ë˜ëŸ‰ ì§€í‘œ
            "volume": ["volume_ratio", "volume_sma_10", "volume_sma_20"],
            "vwap": ["vwap"],
            
            # ë³€ë™ì„± ì§€í‘œ
            "atr": ["atr"],
            "volatility": ["volatility_10", "volatility_20"]
        }
        
        print(f"âœ… ModelTrainer ì´ˆê¸°í™” ì™„ë£Œ: {symbol}")
    
    def start_training(self, 
                      selected_indicators: Dict[str, bool],
                      training_params: Dict,
                      progress_callback: Optional[Callable] = None) -> bool:
        """ë¹„ë™ê¸° í•™ìŠµ ì‹œì‘"""
        if self.is_training:
            print("âŒ ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return False
        
        try:
            # í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”
            self.training_status = {
                "status": "running",
                "current_epoch": 0,
                "total_epochs": training_params.get("epochs", 100),
                "current_batch": 0,
                "total_batches": 0,
                "metrics": {"loss": 0.0, "accuracy": 0.0, "val_loss": 0.0, "val_accuracy": 0.0},
                "start_time": datetime.now(),
                "progress_callback": progress_callback
            }
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í•™ìŠµ ì‹¤í–‰
            self.training_thread = threading.Thread(
                target=self._train_model_async,
                args=(selected_indicators, training_params),
                daemon=True
            )
            
            self.is_training = True
            self.training_thread.start()
            
            print("ğŸš€ AI ëª¨ë¸ í•™ìŠµì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
            self.training_status["status"] = "failed"
            return False
    
    def stop_training(self) -> bool:
        """í•™ìŠµ ì¤‘ì§€"""
        if not self.is_training:
            return False
        
        try:
            self.is_training = False
            self.training_status["status"] = "stopped"
            
            if self.training_thread and self.training_thread.is_alive():
                # ìŠ¤ë ˆë“œê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                self.training_thread.join(timeout=5)
            
            print("â¹ï¸ AI ëª¨ë¸ í•™ìŠµì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_training_status(self) -> Dict:
        """í˜„ì¬ í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
        return self.training_status.copy()
    
    def _train_model_async(self, selected_indicators: Dict[str, bool], training_params: Dict):
        """ë¹„ë™ê¸° í•™ìŠµ ì‹¤í–‰ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            print("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
            self._update_progress_callback("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # 1. ë°ì´í„° ìˆ˜ì§‘
            training_days = training_params.get("training_days", 365)
            interval = training_params.get("interval", "15")
            
            df = self.data_collector.collect_historical_data(interval=interval, days=training_days)
            if df is None or len(df) < 1000:
                raise Exception("ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
            print("ğŸ”§ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...")
            self._update_progress_callback("ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...")
            df = self.data_collector.calculate_technical_indicators(df)
            
            # 3. ì„ íƒëœ ì§€í‘œë§Œ ì¶”ì¶œ
            print("ğŸ“‹ ì„ íƒëœ ì§€í‘œ ì¶”ì¶œ ì¤‘...")
            feature_data = self._prepare_features(df, selected_indicators)
            
            # 4. ë¼ë²¨ë§ (ì¶”ì„¸ ì „í™˜ ê°ì§€)
            print("ğŸ·ï¸ ë¼ë²¨ ìƒì„± ì¤‘...")
            labels = self._create_labels(df)
            
            # 5. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            print("ğŸ“ˆ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            X, y, scaler = self._prepare_training_data(feature_data, labels, training_params)
            
            # 6. ëª¨ë¸ êµ¬ì„±
            print("ğŸ§  ëª¨ë¸ êµ¬ì„± ì¤‘...")
            model = self._build_model(X.shape, training_params)
            
            # 7. í•™ìŠµ ì‹¤í–‰
            print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            self._update_progress_callback("ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            history = self._train_with_progress(model, X, y, training_params)
            
            # 8. ëª¨ë¸ í‰ê°€
            print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
            accuracy = self._evaluate_model(model, X, y)
            
            # 9. ëª¨ë¸ ì €ì¥
            print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
            model_name = self.model_manager.generate_model_name()
            
            model_data = {
                "parameters": training_params,
                "indicators": selected_indicators,
                "training_duration": (datetime.now() - self.training_status["start_time"]).total_seconds(),
                "feature_columns": list(feature_data.columns),
                "data_shape": X.shape
            }
            
            # ëª¨ë¸ íŒŒì¼ ì €ì¥
            model_path = Path("models") / f"{model_name}.h5"
            scaler_path = Path("models") / f"{model_name}_scaler.pkl"
            
            model.save(model_path)
            with open(scaler_path, 'wb') as f:
                pickle.dump(scaler, f)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.model_manager.save_model(model_name, model_data, accuracy)
            
            # í•™ìŠµ ì™„ë£Œ
            self.training_status["status"] = "completed"
            self.training_status["metrics"]["accuracy"] = accuracy
            
            print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {accuracy:.3f}")
            self._update_progress_callback(f"í•™ìŠµ ì™„ë£Œ! ì •í™•ë„: {accuracy:.1%}")
            
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.training_status["status"] = "failed"
            self._update_progress_callback(f"í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        
        finally:
            self.is_training = False
    
    def _prepare_features(self, df: pd.DataFrame, selected_indicators: Dict[str, bool]) -> pd.DataFrame:
        """ì„ íƒëœ ì§€í‘œë§Œ ì¶”ì¶œí•˜ì—¬ íŠ¹ì„± ë°ì´í„° ìƒì„±"""
        feature_columns = []
        
        print("ğŸ“‹ ì„ íƒëœ ì§€í‘œ:")
        for indicator, selected in selected_indicators.items():
            if selected and indicator in self.indicator_mapping:
                columns = self.indicator_mapping[indicator]
                # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¶”ê°€
                existing_columns = [col for col in columns if col in df.columns]
                feature_columns.extend(existing_columns)
                print(f"   âœ… {indicator}: {len(existing_columns)}ê°œ ì»¬ëŸ¼")
            elif selected:
                print(f"   âš ï¸ {indicator}: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€í‘œ")
        
        if not feature_columns:
            raise Exception("ì„ íƒëœ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì§€í‘œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        print(f"ğŸ“Š ì´ íŠ¹ì„± ê°œìˆ˜: {len(feature_columns)}ê°œ")
        
        # ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜
        feature_columns = list(set(feature_columns))
        return df[feature_columns].dropna()
    
    def _create_labels(self, df: pd.DataFrame, 
                      future_window: int = 4,  # 1ì‹œê°„ í›„ (15ë¶„ë´‰ ê¸°ì¤€)
                      significant_change: float = 0.005) -> pd.Series:
        """ì¶”ì„¸ ì „í™˜ ë¼ë²¨ ìƒì„± (NHBot íŠ¹í™”)"""
        
        # ê¸°ë³¸ ê°€ê²© ë³€í™” ê³„ì‚°
        future_prices = df['close'].shift(-future_window)
        price_change = (future_prices - df['close']) / df['close']
        
        # MACD ê¸°ë°˜ ì¶”ì„¸ ì „í™˜ ê°ì§€
        macd_signal = np.where(df['macd'] > df['macd_signal'], 1, -1)
        macd_change = np.diff(macd_signal, prepend=macd_signal[0])
        
        # ì˜ë¯¸ìˆëŠ” ì¶”ì„¸ ì „í™˜ ì¡°ê±´
        # 1. MACD ì‹ í˜¸ ì „í™˜ì´ ìˆìŒ
        # 2. ê°€ê²© ë³€í™”ê°€ ìœ ì˜ë¯¸í•¨ (0.5% ì´ìƒ)
        # 3. RSIê°€ ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ì˜ì—­ì— ìˆì§€ ì•ŠìŒ (30-70 ë²”ìœ„)
        
        trend_reversal = (
            (abs(macd_change) > 0) &  # MACD ì‹ í˜¸ ì „í™˜
            (abs(price_change) > significant_change) &  # ìœ ì˜ë¯¸í•œ ê°€ê²© ë³€í™”
            (df['rsi_14'] > 30) & (df['rsi_14'] < 70) &  # ì¤‘ë¦½ ì˜ì—­
            (df['bb_position'] > 0.2) & (df['bb_position'] < 0.8)  # ë³¼ë¦°ì €ë°´ë“œ ì¤‘ë¦½
        )
        
        # ë¼ë²¨ ìƒì„±: 1(ë§¤ë§¤ í—ˆìš©), 0(ë§¤ë§¤ ê¸ˆì§€)
        labels = trend_reversal.astype(int)
        
        positive_ratio = labels.sum() / len(labels)
        print(f"ğŸ·ï¸ ë¼ë²¨ ë¶„í¬: ë§¤ë§¤ í—ˆìš© {positive_ratio:.1%}, ë§¤ë§¤ ê¸ˆì§€ {1-positive_ratio:.1%}")
        
        return labels
    
    def _prepare_training_data(self, feature_data: pd.DataFrame, labels: pd.Series, 
                              training_params: Dict) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:
        """í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë³€í™˜)"""
        
        sequence_length = training_params.get("sequence_length", 60)
        
        # ë°ì´í„° ì •ê·œí™”
        scaler = MinMaxScaler()
        scaled_features = scaler.fit_transform(feature_data)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X, y = [], []
        
        for i in range(sequence_length, len(scaled_features)):
            if not self.is_training:  # ì¤‘ì§€ ìš”ì²­ í™•ì¸
                break
                
            X.append(scaled_features[i-sequence_length:i])
            y.append(labels.iloc[i])
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í˜•íƒœ: X={X.shape}, y={y.shape}")
        
        return X, y, scaler
    
    def _build_model(self, input_shape: Tuple, training_params: Dict) -> tf.keras.Model:
        """LSTM ëª¨ë¸ êµ¬ì„±"""
        
        model = Sequential([
            # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(64, return_sequences=True, input_shape=input_shape[1:]),
            Dropout(0.2),
            BatchNormalization(),
            
            # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´  
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            BatchNormalization(),
            
            # Dense ë ˆì´ì–´ë“¤
            Dense(32, activation='relu'),
            Dropout(0.3),
            Dense(16, activation='relu'),
            Dropout(0.2),
            
            # ì¶œë ¥ ë ˆì´ì–´ (ì´ì§„ ë¶„ë¥˜)
            Dense(1, activation='sigmoid')
        ])
        
        # ì»´íŒŒì¼
        learning_rate = training_params.get("learning_rate", 0.001)
        optimizer = Adam(learning_rate=learning_rate)
        
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        print("ğŸ§  ëª¨ë¸ êµ¬ì¡°:")
        model.summary()
        
        return model
    
    def _train_with_progress(self, model: tf.keras.Model, X: np.ndarray, y: np.ndarray, 
                           training_params: Dict) -> tf.keras.callbacks.History:
        """ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ í•™ìŠµ ì‹¤í–‰"""
        
        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
        validation_split = training_params.get("validation_split", 20) / 100
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=validation_split, random_state=42, stratify=y
        )
        
        # ë°°ì¹˜ í¬ê¸°
        batch_size = training_params.get("batch_size", 32)
        epochs = training_params.get("epochs", 100)
        
        # ì´ ë°°ì¹˜ ìˆ˜ ê³„ì‚°
        total_batches = len(X_train) // batch_size
        self.training_status["total_batches"] = total_batches
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7),
            TrainingProgressCallback(self)  # ì»¤ìŠ¤í…€ ì½œë°±
        ]
        
        # í•™ìŠµ ì‹¤í–‰
        history = model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=0  # ì§„í–‰ë¥ ì€ ì»¤ìŠ¤í…€ ì½œë°±ì—ì„œ ì²˜ë¦¬
        )
        
        return history
    
    def _evaluate_model(self, model: tf.keras.Model, X: np.ndarray, y: np.ndarray) -> float:
        """ëª¨ë¸ í‰ê°€"""
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred_proba = model.predict(X, verbose=0)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = accuracy_score(y, y_pred)
        
        # ìƒì„¸ ë¦¬í¬íŠ¸
        print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
        print(f"   ì •í™•ë„: {accuracy:.3f}")
        print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(y, y_pred, target_names=['ë§¤ë§¤ ê¸ˆì§€', 'ë§¤ë§¤ í—ˆìš©']))
        
        return accuracy
    
    def _update_progress_callback(self, message: str):
        """ì§„í–‰ë¥  ì½œë°± ì—…ë°ì´íŠ¸"""
        if self.training_status.get("progress_callback"):
            try:
                self.training_status["progress_callback"](message)
            except:
                pass

class TrainingProgressCallback(tf.keras.callbacks.Callback):
    """í•™ìŠµ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì½œë°±"""
    
    def __init__(self, trainer: ModelTrainer):
        super().__init__()
        self.trainer = trainer
    
    def on_epoch_begin(self, epoch, logs=None):
        if not self.trainer.is_training:
            self.model.stop_training = True
            return
            
        self.trainer.training_status["current_epoch"] = epoch + 1
        self.trainer._update_progress_callback(f"ì—í­ {epoch + 1}/{self.trainer.training_status['total_epochs']}")
    
    def on_batch_end(self, batch, logs=None):
        if not self.trainer.is_training:
            self.model.stop_training = True
            return
            
        self.trainer.training_status["current_batch"] = batch + 1
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if logs:
            metrics = self.trainer.training_status["metrics"]
            metrics["loss"] = logs.get("loss", 0)
            metrics["accuracy"] = logs.get("accuracy", 0)
            metrics["val_loss"] = logs.get("val_loss", 0)
            metrics["val_accuracy"] = logs.get("val_accuracy", 0)
    
    def on_epoch_end(self, epoch, logs=None):
        if logs:
            metrics = self.trainer.training_status["metrics"] 
            metrics["loss"] = logs.get("loss", 0)
            metrics["accuracy"] = logs.get("accuracy", 0)
            metrics["val_loss"] = logs.get("val_loss", 0)
            metrics["val_accuracy"] = logs.get("val_accuracy", 0)
            
            print(f"ì—í­ {epoch + 1}: ì†ì‹¤={metrics['loss']:.4f}, ì •í™•ë„={metrics['accuracy']:.3f}")

# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ ModelTrainer í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = ModelTrainer("BTCUSDT")
    
    # ì„ íƒëœ ì§€í‘œ (ì›¹ UIì—ì„œ ë°›ì„ ë°ì´í„°)
    selected_indicators = {
        "price": True,
        "sma": True,
        "ema": True,
        "rsi": True,
        "macd": True,
        "bb": True,
        "stoch": False,  # ìŠ¤í† ìºìŠ¤í‹± ì‚¬ìš© ì•ˆí•¨
        "williams": False,
        "volume": True,
        "atr": True,
        "volatility": True
    }
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„° (ì›¹ UIì—ì„œ ë°›ì„ ë°ì´í„°)
    training_params = {
        "training_days": 30,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 30ì¼ë§Œ
        "epochs": 10,         # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ì—í­ë§Œ
        "batch_size": 32,
        "learning_rate": 0.001,
        "sequence_length": 60,
        "validation_split": 20,
        "interval": "15"
    }
    
    # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
    def progress_callback(message):
        print(f"ğŸ“¢ {message}")
    
    # í•™ìŠµ ì‹œì‘
    success = trainer.start_training(selected_indicators, training_params, progress_callback)
    
    if success:
        print("âœ… í•™ìŠµì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§
        while trainer.is_training:
            status = trainer.get_training_status()
            print(f"ì§„í–‰ë¥ : {status['current_epoch']}/{status['total_epochs']} ì—í­")
            time.sleep(5)
        
        # ìµœì¢… ê²°ê³¼
        final_status = trainer.get_training_status()
        print(f"ğŸ í•™ìŠµ ì™„ë£Œ! ìƒíƒœ: {final_status['status']}")
        
    else:
        print("âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨")
    
    print("\nâœ… ModelTrainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ")